\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{hyperref}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 1. Nonstationary Matérn Kernel: Mathematical Derivation %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Let $x,x'\in\mathbb{R}^2$ be two spatial locations. For each location, we define a local covariance matrix.
% This matrix models the local length scales that vary with position.

\[
\Sigma(x) = \begin{bmatrix}
\sigma_x^2(x) & \rho\,\sigma_x(x)\,\sigma_y(x) \\
\rho\,\sigma_x(x)\,\sigma_y(x) & \sigma_y^2(x)
\end{bmatrix},
\]
where
\[
\sigma_x(x)=L\Bigl({1+\alpha\,\frac{x_1-\bar{x}}{\Delta x}}\Bigr),\quad
\sigma_y(x)=L\Bigl({1+\alpha\,\frac{x_2-\bar{y}}{\Delta y}}\Bigr).
\]
Here:
\begin{itemize}[noitemsep]
    \item $L$ is the base length scale (``base\_lengthscale\_space''),
    \item $\alpha$ is a modulation factor,
    \item $\bar{x},\bar{y}$ are the mean coordinates,
    \item $\Delta x,\Delta y$ are the ranges of the training data.
\end{itemize}

For two points $x$ and $x'$, let
\[
\Sigma_i=\Sigma(x),\quad \Sigma_j=\Sigma(x').
\]
Define the generalized Mahalanobis distance as
\[
Q_{ij} = (x-x')^\top \Bigl({\frac{\Sigma_i+\Sigma_j}{2}}\Bigr)^{-1}(x-x').
\]
Also, define the prefactor
\[
P(x,x')=\frac{\det\bigl(\Sigma(x)\bigr)^{\frac{1}{4}}\det\bigl(\Sigma(x')\bigr)^{\frac{1}{4}}}{\det\!\Bigl({\frac{\Sigma(x)+\Sigma(x')}{2}}\Bigr)^{\frac{1}{2}}}.
\]
Then the nonstationary Matérn kernel is given by
\[
k(x,x')=\sigma_f^2\, P(x,x')\, \frac{1}{\Gamma(\nu)2^{\nu-1}}
\left(\sqrt{2\nu\,Q_{ij}}\right)^{\nu}K_{\nu}\!\Bigl({\sqrt{2\nu\,Q_{ij}}}\Bigr).
\]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 2. Substitution of Smoothness Parameters %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{(a) Exponential Kernel ($\nu=\frac{1}{2}$)}

For $\nu=\frac{1}{2}$, we use the identity
\[
K_{\frac{1}{2}}(z)=\sqrt{\frac{\pi}{2z}}\,e^{-z}.
\]
Also, note $\Gamma\Bigl({\frac{1}{2}}\Bigr)=\sqrt{\pi}$ and $2^{-\frac{1}{2}}=\frac{1}{\sqrt{2}}$. Substituting $\nu=\frac{1}{2}$ into the kernel expression gives:
\[
k_{0.5}(x,x')=\sigma_f^2\,P(x,x')\, \frac{1}{\Gamma\Bigl({\frac{1}{2}}\Bigr)2^{-\frac{1}{2}}}
\left(\sqrt{2\cdot\frac{1}{2}\,Q_{ij}}\right)^{\frac{1}{2}}K_{\frac{1}{2}}\!\Bigl({\sqrt{2\cdot\frac{1}{2}\,Q_{ij}}}\Bigr).
\]
Since $\sqrt{2\cdot\frac{1}{2}\,Q_{ij}}=\sqrt{Q_{ij}}$, we have
\[
k_{0.5}(x,x')=\sigma_f^2\,P(x,x')\, \frac{1}{\sqrt{\pi}/\sqrt{2}}
\left(\sqrt{Q_{ij}}\right)^{\frac{1}{2}}
\sqrt{\frac{\pi}{2\sqrt{Q_{ij}}}}\,e^{-\sqrt{Q_{ij}}}.
\]
In the stationary case (i.e. when $P(x,x')=1$ and $Q_{ij}=\frac{r^2}{\ell^2}$, with $r=\|x-x'\|$), after simplification we obtain:
\[
k_{0.5}(r)\propto \sigma_f^2\,e^{-r/\ell}.
\]
This is the familiar Exponential kernel.

\medskip

\subsection*{(b) Squared Exponential Kernel ($\nu\to\infty$)}

As $\nu\to\infty$, it can be shown that
\[
\lim_{\nu\to\infty}\frac{1}{\Gamma(\nu)2^{\nu-1}}
\left(\sqrt{2\nu}\frac{r}{\ell}\right)^{\nu}K_{\nu}\!\Bigl({\sqrt{2\nu}\frac{r}{\ell}}\Bigr)
=\exp\!\Bigl({-\frac{r^2}{2\ell^2}}\Bigr).
\]
Thus, in the stationary case, the kernel becomes
\[
k_{\text{SE}}(r)=\sigma_f^2\,\exp\!\Bigl({-\frac{r^2}{2\ell^2}}\Bigr),
\]
which is the Squared Exponential (RBF) kernel and is infinitely differentiable.

\medskip

\subsection*{(c) Intermediate Smoothness (e.g., $\nu=2.5$)}

For an intermediate value such as $\nu=2.5$, the kernel expression involves $K_{2.5}$ and, while more complex algebraically, the essential property is that the GP sample paths will be twice mean-square differentiable.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 3. Differentiability Analysis and RKHS %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{Direct Differentiation}

\textbf{Exponential Kernel ($\nu=\frac{1}{2}$):}
In the stationary case, write
\[
k_{0.5}(r)\propto e^{-r/\ell}.
\]
Differentiate with respect to $r$:
\[
k_{0.5}'(r)=-\frac{1}{\ell}\,e^{-r/\ell}.
\]
However, because $r=|x-x'|$, we must consider the directional derivatives:
\[
\lim_{r\to0^+} k_{0.5}'(r)=-\frac{1}{\ell},\quad \lim_{r\to0^-} k_{0.5}'(r)=+\frac{1}{\ell}.
\]
These unequal limits show a discontinuity at $r=0$, so the kernel is not differentiable there. Hence, its RKHS contains only rough functions and the corresponding GP sample paths are non-differentiable.

\medskip

\textbf{Squared Exponential Kernel:}
\[
k_{\text{SE}}(r)=\sigma_f^2\,\exp\!\Bigl({-\frac{r^2}{2\ell^2}}\Bigr).
\]
Differentiate with respect to $r$:
\[
k_{\text{SE}}'(r)=-\frac{\sigma_f^2\,r}{\ell^2}\,\exp\!\Bigl({-\frac{r^2}{2\ell^2}}\Bigr).
\]
At $r=0$, clearly
\[
k_{\text{SE}}'(0)=0.
\]
Higher-order derivatives exist and are continuous, so the SE kernel is infinitely differentiable and its RKHS contains smooth functions; thus, GP sample paths are smooth.

\medskip

\subsection*{Mean-Square Differentiability and the RKHS Perspective}

A Gaussian Process $f(x)$ is \emph{mean-square differentiable} if
\[
\lim_{h\to0}\mathbb{E}\left[\left(\frac{f(x+h)-f(x)}{h}-f'(x)\right)^2\right]=0.
\]
This condition is satisfied if the kernel $k(x,x')$ has continuous mixed partial derivatives, e.g.,
\[
\frac{\partial^2}{\partial x\,\partial x'}k(x,x').
\]
The RKHS $\mathcal{H}_k$ associated with $k$ is the Hilbert space of functions for which the inner product satisfies the reproducing property:
\[
f(x)=\langle f, k(\cdot,x) \rangle_{\mathcal{H}_k}.
\]
If $k$ is $m$-times continuously differentiable, then every function in $\mathcal{H}_k$ is $m$-times differentiable in the mean-square sense. Therefore:
\begin{itemize}
    \item For $\nu=\frac{1}{2}$, the lack of continuous derivative implies the RKHS contains only non-smooth functions, so GP sample paths are rough everywhere.
    \item For $\nu=2.5$, the RKHS functions are twice differentiable, hence the GP sample paths are smoother.
    \item For $\nu\to\infty$, the RKHS comprises infinitely smooth functions, and so are the GP sample paths.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 4. Cholesky Decomposition and GP Inference %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Given training data $\{x_i,y_i\}_{i=1}^N$, construct the kernel matrix
\[
K_{ij}=k(x_i,x_j).
\]
After adding noise variance, we set
\[
K\leftarrow K+\sigma_n^2 I.
\]
Compute the Cholesky decomposition:
\[
K=L\,L^\top,
\]
where $L$ is a lower triangular matrix. To solve
\[
K\,\bm{\alpha}=\bm{y},
\]
we first solve:
\[
L\,\bm{v}=\bm{y},
\]
and then:
\[
L^\top\,\bm{\alpha}=\bm{v}.
\]
This provides a numerically stable solution for
\[
\bm{\alpha}=K^{-1}\bm{y}.
\]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 5. Summary of Detailed Mathematical Work %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Summary:}
\begin{enumerate}[label=(\arabic*)]
    \item \textbf{Kernel Construction:} The nonstationary Matérn kernel is defined by
    \[
    k(x,x')=\sigma_f^2\, P(x,x')\, \frac{1}{\Gamma(\nu)2^{\nu-1}}
    \left(\sqrt{2\nu\,Q_{ij}}\right)^{\nu}K_{\nu}\!\Bigl({\sqrt{2\nu\,Q_{ij}}}\Bigr),
    \]
    with
    \[
    Q_{ij}=(x-x')^\top\Bigl({\frac{\Sigma(x)+\Sigma(x')}{2}}\Bigr)^{-1}(x-x')
    \]
    and
    \[
    P(x,x')=\frac{\det\bigl(\Sigma(x)\bigr)^{\frac{1}{4}}\det\bigl(\Sigma(x')\bigr)^{\frac{1}{4}}}{\det\!\Bigl({\frac{\Sigma(x)+\Sigma(x')}{2}}\Bigr)^{\frac{1}{2}}}.
    \]
    \item \textbf{Substitution of $\nu$:} 
    \begin{itemize}
        \item For $\nu=\frac{1}{2}$, the kernel reduces to 
        \[
        k_{0.5}(r)\propto\sigma_f^2\,e^{-r/\ell},
        \]
        the Exponential kernel, which is not differentiable (its derivative has a discontinuity at $r=0$).
        \item For $\nu\to\infty$, the kernel converges to 
        \[
        k_{\text{SE}}(r)=\sigma_f^2\,e^{-r^2/(2\ell^2)},
        \]
        the Squared Exponential kernel, which is infinitely differentiable.
        \item For intermediate values (e.g. $\nu=2.5$), the kernel yields GP sample paths that are twice mean-square differentiable.
    \end{itemize}
    \item \textbf{Differentiability Analysis:}
    \begin{itemize}
        \item \emph{Exponential Kernel:} With 
        \[
        k_{0.5}(r)\propto e^{-r/\ell},\quad k_{0.5}'(r)=-\frac{1}{\ell}\,e^{-r/\ell},
        \]
        we have 
        \[
        \lim_{r\to0^+}k_{0.5}'(r)=-\frac{1}{\ell},\quad \lim_{r\to0^-}k_{0.5}'(r)=+\frac{1}{\ell}.
        \]
        Thus, the derivative is discontinuous at $r=0$, and the kernel (and hence the GP) is non-differentiable.
        \item \emph{Squared Exponential Kernel:} With
        \[
        k_{\text{SE}}(r)=\sigma_f^2\,\exp\!\Bigl({-\frac{r^2}{2\ell^2}}\Bigr),
        \]
        its derivative 
        \[
        k_{\text{SE}}'(r)=-\frac{\sigma_f^2\,r}{\ell^2}\,\exp\!\Bigl({-\frac{r^2}{2\ell^2}}\Bigr)
        \]
        is continuous for all $r$, ensuring infinite differentiability.
    \end{itemize}
    \item \textbf{RKHS and Mean-Square Differentiability:} 
    The RKHS $\mathcal{H}_k$ associated with a kernel $k$ consists of functions $f$ such that 
    \[
    f(x)=\langle f, k(\cdot,x)\rangle_{\mathcal{H}_k}.
    \]
    If $k(x,x')$ is $m$-times continuously differentiable, then every function in $\mathcal{H}_k$ is $m$-times mean-square differentiable. Therefore:
    \begin{itemize}
        \item For $\nu=\frac{1}{2}$, the discontinuity in the first derivative implies the RKHS contains only rough functions (GP sample paths are non-differentiable).
        \item For higher $\nu$, the RKHS consists of smoother functions, and the GP sample paths become smoother accordingly.
    \end{itemize}
    \item \textbf{GP Inference via Cholesky Decomposition:} 
    For training data $\{x_i, y_i\}_{i=1}^N$, construct the kernel matrix $K$ with $K_{ij}=k(x_i,x_j)$, then add noise:
    \[
    K\leftarrow K+\sigma_n^2 I.
    \]
    Decompose:
    \[
    K = L\,L^\top,
    \]
    and solve
    \[
    L\,\bm{v}=\bm{y},\quad L^\top\,\bm{\alpha}=\bm{v},
    \]
    yielding $\bm{\alpha}=K^{-1}\bm{y}$ in a numerically stable manner.
\end{enumerate}

\end{document}
