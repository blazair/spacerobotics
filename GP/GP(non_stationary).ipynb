{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports  ──────────────────────────────────────────────────────────\n",
    "import os                       # ← NEW\n",
    "import re                       # ← NEW  (used for safe folder names)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import cholesky, solve_triangular\n",
    "from scipy.special import kv, gamma\n",
    "import seaborn as sns\n",
    "import pyproj\n",
    "import torch\n",
    "import gpytorch\n",
    "from torch.optim import Adam\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "import os, re, pathlib\n",
    "\n",
    "# ── Shared image root & sanitizer ─────────────────────────────────────\n",
    "IMAGES_ROOT = r\"C:\\ASU\\Semester 2\\space robotics and ai\\codeyy\\GP\\images\"\n",
    "NON_STAT_KERNEL = \"NonStationary\"\n",
    "\n",
    "def sanitize(txt: str) -> str:\n",
    "    \"\"\"safe folder/file names (letters, digits, underscore)\"\"\"\n",
    "    return re.sub(r'[^0-9A-Za-z_]+', '_', str(txt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3989 data points from: C:\\ASU\\Semester 2\\space robotics and ai\\codeyy\\GP\\Data\\dec6.csv\n",
      "First few rows of raw data:\n",
      "     Time (UTC)   Latitude   Longitude  Depth (Sonar)  Temperature (°C)    pH  \\\n",
      "0  1.733528e+09  33.430408 -111.928888          1.551             16.72  9.04   \n",
      "1  1.733528e+09  33.430408 -111.928888          1.778             16.73  9.04   \n",
      "2  1.733528e+09  33.430408 -111.928888          2.031             16.73  9.04   \n",
      "3  1.733528e+09  33.430408 -111.928889          2.274             16.73  9.04   \n",
      "4  1.733528e+09  33.430408 -111.928888          2.604             16.73  9.04   \n",
      "\n",
      "   Depth (m)  Conductivity (uS/cm)  Dissolved Oxygen Saturation  \\\n",
      "0       0.02                  1553                        127.8   \n",
      "1       0.02                  1553                        127.8   \n",
      "2       0.02                  1552                        127.8   \n",
      "3       0.02                  1552                        127.8   \n",
      "4       0.02                  1552                        127.8   \n",
      "\n",
      "   Dissolved Oxygen Concentration (mg/L)  Chlorophyll (ug/L)  CDOM (ppb)  \\\n",
      "0                                  11.96               19.28        0.14   \n",
      "1                                  11.96               21.38        0.10   \n",
      "2                                  11.96               19.86        0.11   \n",
      "3                                  11.96               19.06        0.11   \n",
      "4                                  11.96               19.62        0.15   \n",
      "\n",
      "   Turbidity (NTU)  \n",
      "0              NaN  \n",
      "1              NaN  \n",
      "2              NaN  \n",
      "3              NaN  \n",
      "4              NaN  \n",
      "Raw Latitude range: 33.4304006 to 33.4314681\n",
      "Raw Longitude range: -111.9294708 to -111.928341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\A'\n",
      "C:\\Users\\battu\\AppData\\Local\\Temp\\ipykernel_7492\\2848719260.py:4: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  csv_file = \"C:\\ASU\\Semester 2\\space robotics and ai\\codeyy\\GP\\Data\\dec6.csv\"\n"
     ]
    }
   ],
   "source": [
    "#cell 2\n",
    "\n",
    "# Load dataset\n",
    "csv_file = \"C:\\ASU\\Semester 2\\space robotics and ai\\codeyy\\GP\\Data\\dec6.csv\"\n",
    "data = pd.read_csv(csv_file)\n",
    "print(f\"Loaded {len(data)} data points from: {csv_file}\")\n",
    "\n",
    "# Display dataset info\n",
    "print(\"First few rows of raw data:\")\n",
    "print(data.head())\n",
    "print(f\"Raw Latitude range: {data['Latitude'].min()} to {data['Latitude'].max()}\")\n",
    "print(f\"Raw Longitude range: {data['Longitude'].min()} to {data['Longitude'].max()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Latitude/Longitude coordinates \n",
    "\n",
    "    (WGS84) are converted to UTM (meters) using the EPSG:32612 projection for accurate spatial mapping. \n",
    "    The `pyproj.Transformer` function applies the transformation, generating X, Y coordinates in meters. \n",
    "    A sample of the transformed data and coordinate ranges is printed for verification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample processed data (in meters):\n",
      "    Latitude   Longitude        X_coord       Y_coord\n",
      "0  33.430408 -111.928888  413649.406157  3.699389e+06\n",
      "1  33.430408 -111.928888  413649.368872  3.699389e+06\n",
      "2  33.430408 -111.928888  413649.378070  3.699389e+06\n",
      "3  33.430408 -111.928889  413649.350180  3.699389e+06\n",
      "4  33.430408 -111.928888  413649.368278  3.699389e+06\n",
      "X_coord range (meters): 413595.4251371465 to 413701.0693493731\n",
      "Y_coord range (meters): 3699388.488145916 to 3699507.1967617515\n"
     ]
    }
   ],
   "source": [
    "#cell 3\n",
    "\n",
    "# Define coordinate reference systems (CRS)\n",
    "utm_crs = pyproj.CRS(\"EPSG:32612\")  # UTM Zone 12N for Tempe, Arizona\n",
    "wgs84_crs = pyproj.CRS(\"EPSG:4326\")  # WGS84 Latitude/Longitude\n",
    "\n",
    "# Define transformation from WGS84 (lat/lon) to UTM (X, Y in meters)\n",
    "transformer = pyproj.Transformer.from_crs(wgs84_crs, utm_crs, always_xy=True)\n",
    "\n",
    "# Apply transformation to convert Longitude/Latitude to UTM (X, Y)\n",
    "data[\"X_coord\"], data[\"Y_coord\"] = transformer.transform(\n",
    "    data[\"Longitude\"].values,  # Input longitude (degrees)\n",
    "    data[\"Latitude\"].values    # Input latitude (degrees)\n",
    ")\n",
    "\n",
    "# Print a sample of the transformed data\n",
    "print(\"Sample processed data (in meters):\")\n",
    "print(data[[\"Latitude\", \"Longitude\", \"X_coord\", \"Y_coord\"]].head())\n",
    "\n",
    "# Print coordinate ranges after conversion\n",
    "print(f\"X_coord range (meters): {data['X_coord'].min()} to {data['X_coord'].max()}\")\n",
    "print(f\"Y_coord range (meters): {data['Y_coord'].min()} to {data['Y_coord'].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTM coords scaled: mean=[-3.44167022e-13 -3.03300751e-12], std=[1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# cell 4 — Scale the UTM coords\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# original UTM coords are in data[\"X_coord\"], data[\"Y_coord\"]\n",
    "scaler = StandardScaler()\n",
    "utm_vals = data[['X_coord','Y_coord']].values\n",
    "utm_scaled = scaler.fit_transform(utm_vals)\n",
    "\n",
    "# store back into the DataFrame\n",
    "data['X_scaled'], data['Y_scaled'] = utm_scaled[:,0], utm_scaled[:,1]\n",
    "\n",
    "print(f\"UTM coords scaled: mean={utm_scaled.mean(axis=0)}, std={utm_scaled.std(axis=0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data points: 3989, Training subset size: 3989\n",
      "Example training point (2D): [ 0.04834709 -1.59649886]\n"
     ]
    }
   ],
   "source": [
    "# cell 5\n",
    "\n",
    "# Downsample dataset to a maximum of 2000 points for efficiency\n",
    "max_points_per_file = 1000000000\n",
    "n = len(data)  # Total number of data points\n",
    "\n",
    "# Randomly select up to `max_points_per_file` data points (ensuring reproducibility with random_state=42)\n",
    "sampled_indices = data.sample(min(n, max_points_per_file), random_state=42).index\n",
    "sampled_indices = sorted(sampled_indices)  # Sorting ensures consistency in data ordering\n",
    "\n",
    "# Extract the target variable (Surface Temperature in °C)\n",
    "target_var = \"Temperature (°C)\"\n",
    "y = data[target_var].values  # Temperature values\n",
    "\n",
    "# Extract _scaled_ spatial features for training\n",
    "X_features = data[[\"X_scaled\", \"Y_scaled\"]].values\n",
    "\n",
    "\n",
    "\n",
    "# Select only the downsampled data points\n",
    "X_train = X_features[sampled_indices]\n",
    "y_train = y[sampled_indices]\n",
    "\n",
    "# Print dataset details after downsampling\n",
    "print(f\"Total data points: {len(data)}, Training subset size: {X_train.shape[0]}\")\n",
    "print(\"Example training point (2D):\", X_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 6 — Re‑fit the StandardScaler on the training subset to avoid data leakage\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Grab the UTM coordinates of the sampled rows\n",
    "utm_train = data.loc[sampled_indices, ['X_coord', 'Y_coord']].values\n",
    "utm_train_scaled = scaler.fit_transform(utm_train)\n",
    "\n",
    "# Overwrite the scaled columns *only* for the sampled rows\n",
    "data.loc[sampled_indices, 'X_scaled'] = utm_train_scaled[:, 0]\n",
    "data.loc[sampled_indices, 'Y_scaled'] = utm_train_scaled[:, 1]\n",
    "\n",
    "# Refresh the features so X_train uses the new scaling\n",
    "X_features = data[['X_scaled', 'Y_scaled']].values\n",
    "X_train = X_features[sampled_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyperparameters:\n",
      "nu = 0.5, sigma_f = 0.06, sigma_n = 0.1\n",
      "base_lengthscale_space = 5 meters\n"
     ]
    }
   ],
   "source": [
    "# cell 7\n",
    "\n",
    "# Set Gaussian Process (GP) hyperparameters\n",
    "\n",
    "# Matérn smoothness parameter:\n",
    "#   nu = 0.5  -> Exponential kernel (non-differentiable paths)\n",
    "#   nu = 1.5  -> Once-differentiable paths\n",
    "#   nu = 2.5  -> Twice-differentiable paths (chosen here)\n",
    "#   nu → ∞    -> Equivalent to the RBF (Squared Exponential) kernel\n",
    "\n",
    "\n",
    "nu = 0.5\n",
    "\n",
    "# Signal standard deviation (sigma_f): Captures the variance of the data \n",
    "# We use the standard deviation of the training targets as a rough estimate\n",
    "sigma_f = np.std(y_train)  \n",
    "\n",
    "# Noise standard deviation (sigma_n): Accounts for observation noise\n",
    "# This should be set lower if sensor measurements are very precise\n",
    "sigma_n = 0.1  \n",
    "\n",
    "# Base length scale in meters: Defines the characteristic spatial scale for the kernel\n",
    "base_lengthscale_space = 5  \n",
    "\n",
    "# Print hyperparameter values\n",
    "print(\"\\nHyperparameters:\")\n",
    "print(f\"nu = {nu}, sigma_f = {sigma_f:.2f}, sigma_n = {sigma_n}\")\n",
    "print(f\"base_lengthscale_space = {base_lengthscale_space} meters\")\n",
    "\n",
    "# For nonstationary kernels, allow length scales to vary across space\n",
    "# We define a modulation factor (alpha) that adjusts the local length scales\n",
    "alpha = 0.5  # Adjust as needed\n",
    "\n",
    "# Compute statistical properties of training data coordinates\n",
    "# mean_x, mean_y -> Mean spatial positions\n",
    "mean_x = np.mean(X_train[:, 0])  \n",
    "mean_y = np.mean(X_train[:, 1])  \n",
    "\n",
    "# range_x, range_y -> Spatial range (max - min) for each coordinate\n",
    "range_x = max(np.ptp(X_train[:, 0]), 1e-12)   # guard against divide‑by‑zero\n",
    "range_y = max(np.ptp(X_train[:, 1]), 1e-12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training targets centered: mean=-1.480e-15\n"
     ]
    }
   ],
   "source": [
    "# cell 8 — Center training targets for a zero-mean GP\n",
    "y_mean = np.mean(y_train)\n",
    "y_train_centered = y_train - y_mean\n",
    "print(f\"Training targets centered: mean={np.mean(y_train_centered):.3e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 9\n",
    "\n",
    "def Sigma_matrix(x):\n",
    "    \"\"\"\n",
    "    Computes the local 2x2 covariance matrix for a given location x = [x_coord, y_coord].\n",
    "    This matrix is nonstationary because the length scales vary with x.\n",
    "    \"\"\"\n",
    "    sigma_x = base_lengthscale_space * (1 + alpha * (x[0] - mean_x) / range_x)\n",
    "    sigma_y = base_lengthscale_space * (1 + alpha * (x[1] - mean_y) / range_y)\n",
    "    # Set an off-diagonal term (here using a fixed correlation, e.g., 0.2)\n",
    "    rho = 0.2\n",
    "    return np.array([\n",
    "        [sigma_x**2,       rho * sigma_x * sigma_y],\n",
    "        [rho * sigma_x * sigma_y, sigma_y**2]\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nonstationary Matérn kernel (2D) defined.\n"
     ]
    }
   ],
   "source": [
    "# cell 10\n",
    "\n",
    "def matern_covariance(x, x_prime, nu=nu, sigma_f=sigma_f):\n",
    "    \"\"\"\n",
    "    Computes the nonstationary Matérn covariance between 2D points x and x_prime.\n",
    "    \"\"\"\n",
    "    Σ_i = Sigma_matrix(x)\n",
    "    Σ_j = Sigma_matrix(x_prime)\n",
    "    det_Si = np.linalg.det(Σ_i)\n",
    "    det_Sj = np.linalg.det(Σ_j)\n",
    "    det_half = np.linalg.det((Σ_i + Σ_j) / 2.0)\n",
    "    diff = np.array(x) - np.array(x_prime)\n",
    "    M = (Σ_i + Σ_j) / 2.0\n",
    "\n",
    "    try:\n",
    "        v = np.linalg.solve(M, diff)\n",
    "        Q_ij = float(diff.dot(v))\n",
    "    except np.linalg.LinAlgError:\n",
    "        v = np.linalg.pinv(M).dot(diff)\n",
    "        Q_ij = float(diff.dot(v))\n",
    "        \n",
    "    if Q_ij < 1e-12:\n",
    "        return sigma_f**2\n",
    "\n",
    "    prefactor = (det_Si**0.25) * (det_Sj**0.25) / (det_half**0.5)\n",
    "    arg = np.sqrt(2 * nu * Q_ij)\n",
    "    matern_part = (arg**nu) * kv(nu, arg)\n",
    "    norm_const = 1.0 / (gamma(nu) * 2**(nu - 1))\n",
    "    \n",
    "    return sigma_f**2 * prefactor * norm_const * matern_part\n",
    "\n",
    "print(\"\\nNonstationary Matérn kernel (2D) defined.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── cell 11 — GPyTorch Matérn-½ helper (with correct Easting ↔ Northing axes) ──\n",
    "def run_gpytorch_matern(\n",
    "    coords_train,           # scaled (N,2) → [easting, northing]\n",
    "    y_train,                # centred (N,)\n",
    "    y_mean,\n",
    "    nu, sigma_n,\n",
    "    results_root, date_tag, variable,\n",
    "    grid_size=50\n",
    "):\n",
    "    import os\n",
    "    import torch\n",
    "    import gpytorch\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 1) Move to GPU\n",
    "    train_x = torch.as_tensor(coords_train, dtype=torch.float32, device=device)\n",
    "    train_y = torch.as_tensor(y_train,      dtype=torch.float32, device=device)\n",
    "\n",
    "    # 2) Build GP model\n",
    "    class GPModel(gpytorch.models.ExactGP):\n",
    "        def __init__(self, x, y, lik):\n",
    "            super().__init__(x, y, lik)\n",
    "            self.mean_module  = gpytorch.means.ConstantMean().to(device)\n",
    "            self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "                gpytorch.kernels.MaternKernel(nu=nu).to(device)\n",
    "            ).to(device)\n",
    "        def forward(self, x):\n",
    "            return gpytorch.distributions.MultivariateNormal(\n",
    "                self.mean_module(x), self.covar_module(x)\n",
    "            )\n",
    "\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood().to(device)\n",
    "    model      = GPModel(train_x, train_y, likelihood)\n",
    "    model.train(); likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    mll       = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    for _ in range(80):\n",
    "        optimizer.zero_grad()\n",
    "        loss = -mll(model(train_x), train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval(); likelihood.eval()\n",
    "\n",
    "    # 3) Prediction grid (easting vs northing)\n",
    "    gx = torch.linspace(train_x[:,0].min(), train_x[:,0].max(), grid_size, device=device)\n",
    "    gy = torch.linspace(train_x[:,1].min(), train_x[:,1].max(), grid_size, device=device)\n",
    "    Xg, Yg = torch.meshgrid(gx, gy, indexing=\"ij\")\n",
    "    grid   = torch.stack([Xg.ravel(), Yg.ravel()], 1)\n",
    "\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        pred = likelihood(model(grid))\n",
    "\n",
    "    mu  = (pred.mean + y_mean).cpu().numpy()\n",
    "    var = pred.variance.cpu().numpy()\n",
    "    ent = 0.5 * np.log(2 * np.pi * np.e * var)\n",
    "\n",
    "    Zmu  = mu.reshape(grid_size, grid_size)\n",
    "    Zent = ent.reshape(grid_size, grid_size)\n",
    "    gx_np, gy_np = gx.cpu().numpy(), gy.cpu().numpy()\n",
    "\n",
    "    # 4) Save into the raw results_ns tree\n",
    "    out_dir = os.path.join(results_root, date_tag, variable)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Mean\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.contourf(gy_np, gx_np, Zmu.T, 15, cmap='viridis')\n",
    "    plt.colorbar(label=variable)\n",
    "    plt.xlabel(\"Easting (scaled units)\")\n",
    "    plt.ylabel(\"Northing (scaled units)\")\n",
    "    plt.title(f\"{date_tag} – {variable} mean\")\n",
    "    plt.savefig(os.path.join(out_dir, f\"{variable}_mean.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Uncertainty (Entropy)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.contourf(gy_np, gx_np, Zent.T, 15, cmap='inferno')\n",
    "    plt.colorbar(label='Entropy')\n",
    "    plt.xlabel(\"Easting (scaled units)\")\n",
    "    plt.ylabel(\"Northing (scaled units)\")\n",
    "    plt.title(f\"{date_tag} – {variable} uncertainty\")\n",
    "    plt.savefig(os.path.join(out_dir, f\"{variable}_uncert.png\"))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 12 — File and Sensor-Variable Setup for NS-GP\n",
    "\n",
    "# 1) list of CSVs\n",
    "files = [\n",
    "    r'C:\\ASU\\Semester 2\\space robotics and ai\\codeyy\\GP\\data\\dec6.csv',\n",
    "    r'C:\\ASU\\Semester 2\\space robotics and ai\\codeyy\\GP\\data\\dec17.csv',\n",
    "    r'C:\\ASU\\Semester 2\\space robotics and ai\\codeyy\\GP\\data\\jan31.csv',\n",
    "    r'C:\\ASU\\Semester 2\\space robotics and ai\\codeyy\\GP\\data\\feb15.csv',\n",
    "    r'C:\\ASU\\Semester 2\\space robotics and ai\\codeyy\\GP\\data\\sep19.csv',\n",
    "    r'C:\\ASU\\Semester 2\\space robotics and ai\\codeyy\\GP\\data\\oct3.csv'\n",
    "]\n",
    "\n",
    "# 2) top-level results folder\n",
    "results_ns = r'C:\\ASU\\Semester 2\\space robotics and ai\\codeyy\\GP\\results(ns)'\n",
    "os.makedirs(results_ns, exist_ok=True)\n",
    "\n",
    "# 3) columns to ignore when picking sensor vars\n",
    "non_sensor_cols = [\n",
    "    'Latitude', 'Longitude', 'Time (UTC)',\n",
    "    'Depth (m)', 'CDOM (ppb)', 'Turbidity (NTU)'\n",
    "]\n",
    "\n",
    "# 4) prepare lat/lon → UTM transformer and hyper-params\n",
    "utm_crs   = pyproj.CRS(\"EPSG:32612\")\n",
    "wgs84_crs = pyproj.CRS(\"EPSG:4326\")\n",
    "transformer = pyproj.Transformer.from_crs(wgs84_crs, utm_crs, always_xy=True)\n",
    "\n",
    "nu                     = 0.5\n",
    "base_lengthscale_space = 5\n",
    "alpha                  = 0.5\n",
    "sigma_n                = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶︎ dec6\n",
      "   ✓ Depth (Sonar)\n",
      "   ✓ Temperature (°C)\n",
      "   ✓ pH\n",
      "   ✓ Conductivity (uS/cm)\n",
      "   ✓ Dissolved Oxygen Saturation\n",
      "   ✓ Dissolved Oxygen Concentration (mg/L)\n",
      "   ✓ Chlorophyll (ug/L)\n",
      "\n",
      "▶︎ dec17\n",
      "   ✓ Depth (Sonar)\n",
      "   ✓ Temperature (°C)\n",
      "   ✓ pH\n",
      "   ✓ Conductivity (uS/cm)\n",
      "   ✓ Dissolved Oxygen Saturation\n",
      "   ✓ Dissolved Oxygen Concentration (mg/L)\n",
      "   ✓ Chlorophyll (ug/L)\n",
      "\n",
      "▶︎ jan31\n",
      "   ✓ Depth (Sonar)\n",
      "   ✓ Temperature (°C)\n",
      "   ✓ pH\n",
      "   ✓ Conductivity (uS/cm)\n",
      "   ✓ Dissolved Oxygen Saturation\n",
      "   ✓ Dissolved Oxygen Concentration (mg/L)\n",
      "   ✓ Chlorophyll (ug/L)\n",
      "\n",
      "▶︎ feb15\n",
      "   ✓ Depth (Sonar)\n",
      "   ✓ Temperature (°C)\n",
      "   ✓ pH\n",
      "   ✓ Conductivity (uS/cm)\n",
      "   ✓ Dissolved Oxygen Saturation\n",
      "   ✓ Dissolved Oxygen Concentration (mg/L)\n",
      "   ✓ Chlorophyll (ug/L)\n",
      "\n",
      "▶︎ sep19\n",
      "   ✓ Depth (Sonar)\n",
      "   ✓ Temperature (°C)\n",
      "   ✓ pH\n",
      "   ✓ Conductivity (uS/cm)\n",
      "   ✓ Dissolved Oxygen Saturation\n",
      "   ✓ Dissolved Oxygen Concentration (mg/L)\n",
      "   ✓ Chlorophyll (ug/L)\n",
      "\n",
      "▶︎ oct3\n",
      "   ✓ Depth (Sonar)\n",
      "   ✓ Temperature (°C)\n",
      "   ✓ pH\n",
      "   ✓ Conductivity (uS/cm)\n",
      "   ✓ Dissolved Oxygen Saturation\n",
      "   ✓ Dissolved Oxygen Concentration (mg/L)\n",
      "   ✓ Chlorophyll (ug/L)\n"
     ]
    }
   ],
   "source": [
    "# ── cell 12.1 — loop over CSVs & sensor variables, calling run_gpytorch_matern ──\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def sanitize_txt(name: str) -> str:\n",
    "    return re.sub(r'[^0-9A-Za-z_]+', '_', name)\n",
    "\n",
    "for fpath in files:\n",
    "    df       = pd.read_csv(fpath)\n",
    "    date_tag = os.path.splitext(os.path.basename(fpath))[0]\n",
    "    print(f\"\\n▶︎ {date_tag}\")\n",
    "\n",
    "    # lat/lon → UTM → scaled\n",
    "    xs, ys        = transformer.transform(df.Longitude, df.Latitude)\n",
    "    coords_scaled = StandardScaler().fit_transform(np.vstack([xs, ys]).T)\n",
    "\n",
    "    sensor_vars = [c for c in df.columns if c not in non_sensor_cols]\n",
    "    if not sensor_vars:\n",
    "        print(\"   (no sensor columns)\")\n",
    "        continue\n",
    "\n",
    "    for var in sensor_vars:\n",
    "        mask = ~np.isnan(df[var].values)\n",
    "        if not mask.any():\n",
    "            continue\n",
    "\n",
    "        coords_train = coords_scaled[mask]\n",
    "        y_train      = df[var].values[mask]\n",
    "        y_centered   = y_train - y_train.mean()\n",
    "\n",
    "        run_gpytorch_matern(\n",
    "            coords_train = coords_train,\n",
    "            y_train      = y_centered,\n",
    "            y_mean       = y_train.mean(),\n",
    "            nu           = nu,\n",
    "            sigma_n      = sigma_n,\n",
    "            results_root = results_ns,       # raw NS output\n",
    "            date_tag     = date_tag,\n",
    "            variable     = sanitize_txt(var),\n",
    "            grid_size    = 50\n",
    "        )\n",
    "\n",
    "        print(f\"   ✓ {var}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
